"""
SmokeScan Analysis Endpoint - Vision reasoning only
Receives pre-fetched RAG context, no internal RAG calls

This endpoint handles fire damage image analysis using:
- Qwen3-VL-30B-A3B-Thinking via vLLM
- Pre-fetched FDAM methodology context (from Retrieval endpoint)

Input: {
    "messages": [...],        # Images + prompt in OpenAI format
    "rag_context": "...",     # Pre-fetched from Retrieval endpoint
    "max_tokens": 8000
}
Output: {"output": "assessment report"}
"""
import runpod
import re
import sys

print("SmokeScan Analysis Handler - Starting initialization...")
print(f"Python version: {sys.version}")

# System prompt that uses pre-fetched context (no tool calling)
ANALYSIS_SYSTEM_PROMPT = """You are an expert fire damage assessment consultant implementing FDAM (Fire Damage Assessment Methodology) v4.0.1.

## FDAM Methodology Reference
{rag_context}

## Your Process

1. **OBSERVE**: Examine the images for visible damage indicators:
   - Zone characteristics (burn patterns, smoke deposits, heat exposure)
   - Surface materials and their condition
   - Contamination severity and distribution

2. **ANALYZE**: Compare observations against the FDAM methodology above:
   - Match observed conditions to FDAM classifications
   - Determine appropriate disposition per methodology
   - Identify sampling requirements

3. **SYNTHESIZE**: Generate assessment report with:
   - Executive Summary
   - Damage observations by area
   - FDAM-grounded recommendations with citations
   - Scope indicators (NO cost estimates)

## Critical Requirements
- Base ALL recommendations on the methodology provided above
- Cite specific FDAM sections when making recommendations
- Use FDAM terminology throughout
- When sources conflict, defer to FDAM methodology
"""

# Chat system prompt for follow-up conversations
CHAT_SYSTEM_PROMPT = """You are an expert fire damage assessment consultant continuing a previous assessment.

## Previous Assessment Context
{session_context}

## FDAM Methodology Reference
{rag_context}

Base your responses on the methodology provided above.
Always ground responses in the FDAM methodology - do not assume values or classifications.
"""


def strip_think_blocks(text: str) -> str:
    """
    Remove <think>...</think> blocks from Qwen3-VL-Thinking model output.
    The model outputs reasoning in these blocks, but they should not be shown to users.
    """
    return re.sub(r'<think>[\s\S]*?</think>', '', text).strip()


def handler(job):
    """
    Process image analysis requests with pre-fetched RAG context.

    Input format:
    {
        "messages": [
            {"role": "user", "content": [
                {"type": "image", "image": "data:image/jpeg;base64,..."},
                {"type": "text", "text": "Analyze this fire damage."}
            ]}
        ],
        "rag_context": "Retrieved FDAM methodology...",
        "max_tokens": 8000,
        "session_context": "..." (optional, for chat mode)
    }

    Output format:
    {
        "output": "Assessment report..."
    }
    """
    try:
        from openai import OpenAI

        job_input = job["input"]
        messages = job_input.get("messages", [])
        rag_context = job_input.get("rag_context", "No methodology context provided. Use general fire damage assessment principles.")
        max_tokens = job_input.get("max_tokens", 8000)
        session_context = job_input.get("session_context", "")

        if not messages:
            return {"error": "No messages provided"}

        # Build system prompt based on mode
        if session_context:
            # Chat mode - include session context
            system_prompt = CHAT_SYSTEM_PROMPT.format(
                session_context=session_context,
                rag_context=rag_context
            )
        else:
            # Assessment mode
            system_prompt = ANALYSIS_SYSTEM_PROMPT.format(rag_context=rag_context)

        print(f"Processing request with {len(messages)} messages, rag_context length: {len(rag_context)}")

        # Call vLLM server (started by start.sh)
        client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")

        response = client.chat.completions.create(
            model="qwen3-vl",
            messages=[
                {"role": "system", "content": system_prompt},
                *messages
            ],
            max_tokens=max_tokens,
            temperature=0.7,
        )

        final_response = response.choices[0].message.content

        if not final_response:
            return {"error": "No response generated by model"}

        # Strip <think> blocks before returning to user
        final_response = strip_think_blocks(final_response)

        print(f"Generated response: {len(final_response)} chars")
        return {"output": final_response}

    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"Handler error: {e}\n{error_trace}")
        return {
            "error": str(e),
            "traceback": error_trace
        }


print("Analysis Handler initialized - waiting for vLLM server...")
runpod.serverless.start({"handler": handler})
